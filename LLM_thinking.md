---
title: "The Day \"Just Ask ChatGPT\" Became the Only Answer"
description: "We've replaced thinking with prompting, understanding with copy-pasting, and learning with asking ChatGPT â€” and we're calling it productivity."
date: "2025-09-03"
author: "Rohit Harsh"
tags: ["LLM", "Thinking", "Apple", "PromptEngineering"]
icon: "ðŸ”„"
gradient: "from-blue-100 to-cyan-100"
patternGradient: "from-blue-200/50 to-cyan-200/50"
---

# The Day "Just Ask ChatGPT" Became the Only Answer

Recently, I needed to write a query to pull the data in BigQuery.

A couple of years back, I would have gone to Google, searched the error, read the schema guide or maybe pinged a teammate for help. 

This time? I copied the error, pasted it into ChatGPT, got a solution, copied it back. It worked. I moved on.

I never understood why it failed. I never learned what fixed it. All I cared was the solution. 

And this made me realize that somewhere along the way, I stopped thinking through the problem and I think many of us did. 

---

## The Truth

The LLMs do not think, but what is more worrisome is **Neither are you, anymore.**

We used to work through the problems. We Googled and then we would read different explanations, synthesize them, build mental models. We asked our team-mates and actually listened to their reasoning. The struggle was the point and that's how we learnt and understood the concepts.

Today we go straight to ChatGPT, not for help to think through the issue, but to avoid thinking entirely. We want the quick answer, not the understanding of the concepts.

And all of this feels so efficient, so smart, and the illusion that we are getting more done than ever.

The reality: We are just clicking a send button.

---

## The Cognitive Decline

Like muscles, our cognitive skills weaken when we don't use them. Relying on AI for every task, question, or problem can quietly erode our ability to reason, think creatively, and solve things ourselves.

A recent [study](https://www.pnas.org/doi/abs/10.1073/pnas.2422633122?doi=10.1073%2Fpnas.2422633122&mod=ANLink) gives us an unsettling glimpse of this phenomenon. Students using ChatGPT performed **48%** better than their counterparts who did not have access to the AI. However, the same group of students performed **17%** lower when the AI access is taken away. 

Another [study](https://www.psychologicalscience.org/news/releases/want-to-solve-a-problem-dont-just-use-your-brains-but-your-bodies-too.html) reminds us that problem solving is not just the work of the brain but also our body. We use our hands, our posture, even our gestures to think, and, when we delegate to AI too quickly, we skip this process and lose the deeper engagement. 

---

## "Just Ask ChatGPT" Is the New Intellectual Laziness

I catch myself doing it constantly now:
- Need to understand a concept? ChatGPT explains it
- Writing an email? ChatGPT drafts it  
- Solving a problem? ChatGPT provides the solution
- Having an opinion? ChatGPT forms it for me

I don't read documentation anymore. I don't Google to search for the answers. I ask ChatGPT to summarize them.

The worst part? When someone asks me to explain something I "learned" this way, I can't. Because I never actually learned it. I just got past it.

---

## Three lies we keep telling ourselves

**Lie #1: "I'm using AI as a tool, like a calculator"**

Calculators just compute, but you still want to figure out what to compute and why, but AI generates entire solutions.

**Lie #2: "I'm more productive than ever"**

You're faster at producing mediocre work. All the solutions, all the articles, all the codes mostly feel the same. You have actually outsourced it to ChatGPT. 

**Lie #3: "I'll learn the real skills when I need them"**

No, you won't. Skills atrophy. More importantly, you won't even know what you don't know, and [Dunning-Kruger](https://en.wikipedia.org/wiki/Dunningâ€“Kruger_effect) can kink in. 

---

## The Day Apple Said No (And Why It Matters)

While everyone rushed to build chatbots and put AI on everything, Apple published two papers essentially saying: "These things can't actually reason, and we can prove it."

They tested the latest "reasoning" models, the ones that supposedly "think step-by-step" and "show their work." The study shows that these models face complete accuracy collapse when problems get complex. Even worse: they *appear* to reason more with harder problems, but then suddenly give up, despite having plenty of tokens left to think.

On simple tasks, regular LLMs actually outperform these "reasoning" models. The entire step-by-step thinking process is an illusion.

Read the studies yourself:
- [GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models](https://machinelearning.apple.com/research/gsm-symbolic)
- [The Illusion of Thinking](https://machinelearning.apple.com/research/illusion-of-thinking)

Today, every company is shipping half baked AI features, not because they or their users need them, but somehow they're terrified of being seen as "behind." They're adding chatbots that don't work, AI assistants that nobody uses, all to tick a box that says "we have AI."

And unless you're OpenAI, Google, or Anthropic, unless you're spending billions on compute and training, you are not in the AI business. You are just another customer, another subscriber or just another monthly payment to use someone else's models.

The entire industry is racing to integrate technology they don't control, can't improve, and barely understand. All to avoid looking outdated in earnings calls. 

But in this mad rush, they are also losing the ability to develop talents and create value. 

---

## What Happens When the Learning Stops 

If you're 30 and dependent on AI-assisted work, you have maybe 5 years before one of the two things happens:

1. **AGI arrives**, and you're instantly obsolete because you trained yourself to be a worse version of what it does perfectly.

2. **AGI doesn't arrive**, and companies realize they need people who can actually think, not prompts for LLM. But you've spent years becoming an AI operator instead of developing real expertise.

Either way, you are unprepared.

The people who will win are the creative minds, who are using this time differently. They're going deeper while everyone else goes broader and shallower. They're the ones who can explain WHY the AI's answer is wrong, not just copy-paste it.

---

## What Thinking Actually Looks Like

When you're really thinking, not just recalling information, you can feel the doubts, the feeling that something is off. You experience that uncomfortable moment where your logic doesn't work, where the pieces don't connect, the frustration of not figuring things out.

When you're just remembering, everything feels smooth. The answer appears just like that, no cognitive effort. 

An LLM can never feel what it takes to think. It doesn't experience that productive discomfort of working through a problem. It doesn't have that "aha" moment when things finally click. 

When you outsource thinking to these systems, you don't lose the answer, you lose the entire journey of understanding. And that journey, that struggle, is what makes you valuable.

---

## The Power Game

Five companies now mediate how millions of people think, and they decide what we repeat, what seems obvious, and what seems like a good idea. 

You think you're getting an AI based personal assistant. In reality, you are getting homogenized thought, filtered through the same training data, the same RLHF, the same guardrails.

The real AI apocalypse isn't robots taking over. It's human thought converging to the same averages. Everyone writing with the same voice, solving problems the same way, having the same "insights" because we all have the same teacher.

---

## What You Should Actually Do

**Think before you ask**: To give your brain a workout, try writing down your thoughts, bullet points, anything which comes to your mind. Then ask the AI to build on it. It might feel slow and tiring like returning to the gym after a break, but thats the point. 

**The Explanation Test**: Instead of saying, "Give me the answer", try saying "Guide me through the problem so I can solve this on my own". Request step by step explanations. Once the answer is there, close the chat window and try to explain it to test the retention. 

**Read Actual Books**: Your brain needs to struggle to understand something, not just consume pre-digested insights.

**Solve Problems Backwards**: When facing a unique issue or a question, start by assuming the AI's first suggestion is wrong. Work backwards to understand why. You'll often find it was right, but now you'll know WHY.

---

## The Choice

We're at an inflection point. Not for AI but for us.

Either we maintain our ability to think and understand what these tools actually are. Brilliant pattern matchers that simulate understanding without possessing it, or we atrophy into prompt engineers for systems we don't comprehend, solving problems we can't actually solve, knowing things we don't really know.

"Just ask ChatGPT" has become the default answer to everything. But what happens when ChatGPT doesn't know? What happens when it's confidently wrong? What happens when the problem needs actual understanding, not pattern matching?

More importantly: What happens when you can no longer tell the difference?

The machines aren't going to start thinking.

The question is: Will we stop?

---

*P.S.: If you used AI to read, summarize, or respond to this article, you've proven my point. Notice what just happened: You outsourced your engagement with an argument about not outsourcing your thinking. Sit with that discomfort. That feeling? That's what thinking feels like.*
