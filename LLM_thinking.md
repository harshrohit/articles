---
title: "Your AI Assistant Isn't Thinking (And That Should Worry You)"
description: "A view at why large language models donâ€™t actually thinkâ€”and why pretending they do could cost us more than we realize."
date: "2025-09-03"
author: "Rohit Harsh"
tags: ["contact-center", "modernization", "strategy", "transformation"]
icon: "ğŸ”„"
gradient: "from-blue-100 to-cyan-100"
patternGradient: "from-blue-200/50 to-cyan-200/50"
---

# Your AI Assistant Isn't Thinking (And That Should Worry You)

Everywhere you look today, thereâ€™s a new tool, a new platform, a new app â€” all claiming to use AI. Not because they all need it. But because thatâ€™s what people expect to hear right now.

But behind all these, there is an importnat question which needs to be asked: 

**What does it mean for a machine to "think"? And are we confusing performance with intelligence?**

---

## The Three Types of Opinions People Hold About AI

Talk to enough people, and you'll notice they usually land in one of three groups:

1. **Skeptics**, who believe the current wave is unsustainable and will eventually taper off.
2. **Optimists**, who admit the current tech is overhyped, but believe a future breakthrough will eventually deliver on the promises â€” so itâ€™s worth staying in the game.
3. **Realists**, who believe this technology has real value, but only in narrow, specific tasks. Beyond that? Mostly noise.

I find myself in the third camp. And before you ask â€” yes, I use these tools and for the record everyday. They're great at certain things. But they're not doing what you think they're doing.

Theyâ€™re not thinking.

---

## â€œBut Are You Saying Every CEO Is Wrong?â€

When I say this out loud, I often get asked:  
*â€œIf you're skeptical, are you saying all these CEOs and industry leaders are wrong?â€*

That question reminds me of 2008.  
I started my career during the financial crash. Back then, everyone â€” from top executives to financial media â€” insisted things were fine, stable, and under control. They werenâ€™t.

The crowd is often wrong. And in hindsight, it's easy to see who was thinking clearly and who was just echoing the group.

---

## â€œWhat Are You Doing to Prepare for the AI Future?â€

Another question I hear regularly.  
My answer: **Not much**.

Unless you're building foundational models yourself â€” which requires billions of dollars in compute and data â€” most of us are just onlookers. Weâ€™ll end up as end-users of systems built by a handful of massive companies. Just another subscription tier.

So no, Iâ€™m not redesigning my life or business around the latest LLM. Because unless you control the core, you're just along for the ride.

---

## Everyoneâ€™s Releasing AI Chatbots â€” Few Are Useful

You've probably noticed this too.  
Suddenly, every product update includes a chatbot. Most donâ€™t work well. They donâ€™t understand nuance, they donâ€™t handle edge cases, and users stop using them after the first few tries.

Adding â€œAIâ€ to a feature isnâ€™t the same as making something useful.

---

## Appleâ€™s Cautious Approach

Personally, I am a fan of Apple, not because of iPhone or MAC, but because how they do business and respect how they wait. Today also, they are doing something interesting: actually thinking about thinking. 

Theyâ€™ve published two deep studies on this subject:

- [The Illusion of Thinking (June 2025)](https://machinelearning.apple.com/research/illusion-of-thinking)  
- [Symbolic Reasoning and GSM8K (Oct 2024)](https://machinelearning.apple.com/research/gsm-symbolic)

Both studies unpack the idea that LLMs often appear to reason â€” but are simply assembling patterns theyâ€™ve seen before.

That doesnâ€™t mean Apple wonâ€™t ship AI features. Investor pressure might eventually push them to. But I admire their instinct to question, not just follow.

---

## River Crossing

Hereâ€™s a simple test.

Youâ€™ve probably heard the puzzle:

> You have a wolf, a goat, and a cabbage. You can only carry one at a time across a river. If you leave the wolf alone with the goat, it eats the goat. Same with the goat and the cabbage.

Most LLMs can solve this â€” because itâ€™s been discussed online for years.

But change the story just a little â€” swap the goat with a car, or reorder the instructions â€” and the system breaks. It loses the logic.

Sometimes the error disappears a week later. Why? Not because the machine suddenly â€œlearned,â€ but because someone behind the scenes patched the output or retrained the model.

It doesnâ€™t understand the problem. It just learned a response that looks correct.

/images/blog/llm-thinking/LLM_thinking.png

---

## What *Is* Thinking?

When we think, we:
- Build mental models
- Question assumptions
- Learn from mistakes
- Connect unrelated ideas
- Imagine "what if" scenarios

Thatâ€™s not how LLMs operate. They donâ€™t introspect. They donâ€™t question their assumptions. They canâ€™t.

Theyâ€™re incredibly advanced pattern recognizers. But they donâ€™t *know* theyâ€™re doing anything.

So why is it hard to notice? Because theyâ€™ve gotten *so good* at producing fluent, well-structured answers, we mistake form for depth.

---

## Workarounds Disguised as Progress

The industry knows these models donâ€™t really reason. So itâ€™s found ways to cover up the limitations.

Two popular ones:

- **Chain of Thought (CoT)** â€” where the model â€œshows its workâ€ like a student on a math test.  
  But it can still fabricate logic that *sounds* reasonable without being true.
- **RAG (Retrieval-Augmented Generation)** â€” which lets the model look up facts in a database before answering.  
  Useful, but also an admission that the model itself doesnâ€™t *know* much of anything.

These are clever solutions. But theyâ€™re still workarounds.

---

## Being Late Isnâ€™t Always a Mistake

Apple was late to smartphones.  
Late to tablets.  
Late to wearables.  
Late to generative AI.

But they still became the first company in history to hit $3 trillion in market value.

Why? Because they made fewer wrong bets. They focused on getting it *right*, not getting there *first*.

Thereâ€™s a lesson in that.

---

## Overdependence and Cognitive Decline

Hereâ€™s what is of utmost concern:

People are replacing understanding with shortcuts.  
Instead of learning how systems work or building deep skills, theyâ€™re learning how to write better prompts.

Btw, if AGI does arrive, we wonâ€™t need prompt engineers.  
And if it *doesnâ€™t*, then a generation of surface-level users â€” who canâ€™t solve problems without a tool â€” will be replaceable. Not by AI, but by others who kept learning.

Thereâ€™s no value in being â€œgood at using ChatGPTâ€ if thatâ€™s all you can do.

---

## We Train the Ceiling

LLMs donâ€™t invent new ideas.  
They remix ours.

Until we build something fundamentally different â€” something that *thinks* â€” these models will only be as good as the people using them.

If we stop thinking, creating, or exploring because â€œAI will handle it,â€ weâ€™re just capping the ceiling on our own progress.

---

## The Bottom Line

Your AI assistant is incredible at pattern matching. It's a master of looking intelligent. But it doesn't think, doesn't understand, and doesn't know it exists.

The danger isn't that AI will replace human thinking.
The danger is that we'll forget what thinking actually is â€” and mistake performance for intelligence, fluency for understanding, and pattern matching for wisdom.

Don't let the illusion fool you. Keep thinking. Keep questioning. Keep creating.

Because if we don't, who will train the next generation of models?
