---
title: "Your AI Assistant Isn't Thinking (And That Should Worry You)"
description: "A view at why large language models don’t actually think—and why pretending they do could cost us more than we realize."
date: "2025-09-03"
author: "Rohit Harsh"
tags: ["contact-center", "modernization", "strategy", "transformation"]
icon: "🔄"
gradient: "from-blue-100 to-cyan-100"
patternGradient: "from-blue-200/50 to-cyan-200/50"
---

# Your AI Assistant Isn't Thinking (And That Should Worry You)


Everywhere you look today, there’s a new tool, a new platform, a new app — all claiming to use AI. Not because they all need it. But because that’s what people expect to hear right now.

But behind all these, there is an importnat question which needs to be asked: 

**What does it mean for a machine to "think"? And are we confusing performance with intelligence?**

---

## The Three Types of Opinions People Hold About AI

Talk to enough people, and you'll notice they usually land in one of three groups:

1. **Skeptics**, who believe the current wave is unsustainable and will eventually taper off.
2. **Optimists**, who admit the current tech is overhyped, but believe a future breakthrough will eventually deliver on the promises — so it’s worth staying in the game.
3. **Realists**, who believe this technology has real value, but only in narrow, specific tasks. Beyond that? Mostly noise.

I find myself in the third camp. And before you ask — yes, I use these tools and for the record everyday. They're great at certain things. But they're not doing what you think they're doing.

They’re not thinking.

---

## “But Are You Saying Every CEO Is Wrong?”

When I say this out loud, I often get asked:  
*“If you're skeptical, are you saying all these CEOs and industry leaders are wrong?”*

That question reminds me of 2008.  
I started my career during the financial crash. Back then, everyone — from top executives to financial media — insisted things were fine, stable, and under control. They weren’t.

The crowd is often wrong. And in hindsight, it's easy to see who was thinking clearly and who was just echoing the group.

---

## “What Are You Doing to Prepare for the AI Future?”

Another question I hear regularly.  
My answer: **Not much**.

Unless you're building foundational models yourself — which requires billions of dollars in compute and data — most of us are just onlookers. We’ll end up as end-users of systems built by a handful of massive companies. Just another subscription tier.

So no, I’m not redesigning my life or business around the latest LLM. Because unless you control the core, you're just along for the ride.

---

## Everyone’s Releasing AI Chatbots — Few Are Useful

You've probably noticed this too.  
Suddenly, every product update includes a chatbot. Most don’t work well. They don’t understand nuance, they don’t handle edge cases, and users stop using them after the first few tries.

Adding “AI” to a feature isn’t the same as making something useful.

---

## Apple’s Cautious Approach

Personally, I am a fan of Apple, not because of iPhone or MAC, but because how they do business and respect how they wait. Today also, they are doing something interesting: actually thinking about thinking. 

They’ve published two deep studies on this subject:

- [The Illusion of Thinking (June 2025)](https://machinelearning.apple.com/research/illusion-of-thinking)  
- [Symbolic Reasoning and GSM8K (Oct 2024)](https://machinelearning.apple.com/research/gsm-symbolic)

Both studies unpack the idea that LLMs often appear to reason — but are simply assembling patterns they’ve seen before.

That doesn’t mean Apple won’t ship AI features. Investor pressure might eventually push them to. But I admire their instinct to question, not just follow.

---

## River Crossing

Here’s a simple test.

You’ve probably heard the puzzle:

> You have a wolf, a goat, and a cabbage. You can only carry one at a time across a river. If you leave the wolf alone with the goat, it eats the goat. Same with the goat and the cabbage.

Most LLMs can solve this — because it’s been discussed online for years.

But change the story just a little — swap the goat with a car, or reorder the instructions — and the system breaks. It loses the logic.

Sometimes the error disappears a week later. Why? Not because the machine suddenly “learned,” but because someone behind the scenes patched the output or retrained the model.

It doesn’t understand the problem. It just learned a response that looks correct.

[ChatGPT Answer](/images/blog/llm-thinking/LLM_thinking.png)

---

## What *Is* Thinking?

When we think, we:
- Build mental models
- Question assumptions
- Learn from mistakes
- Connect unrelated ideas
- Imagine "what if" scenarios

That’s not how LLMs operate. They don’t introspect. They don’t question their assumptions. They can’t.

They’re incredibly advanced pattern recognizers. But they don’t *know* they’re doing anything.

So why is it hard to notice? Because they’ve gotten *so good* at producing fluent, well-structured answers, we mistake form for depth.

---

## Workarounds Disguised as Progress

The industry knows these models don’t really reason. So it’s found ways to cover up the limitations.

Two popular ones:

- **Chain of Thought (CoT)** — where the model “shows its work” like a student on a math test.  
  But it can still fabricate logic that *sounds* reasonable without being true.
- **RAG (Retrieval-Augmented Generation)** — which lets the model look up facts in a database before answering.  
  Useful, but also an admission that the model itself doesn’t *know* much of anything.

These are clever solutions. But they’re still workarounds.

---

## Being Late Isn’t Always a Mistake

Apple was late to smartphones.  
Late to tablets.  
Late to wearables.  
Late to generative AI.

But they still became the first company in history to hit $3 trillion in market value.

Why? Because they made fewer wrong bets. They focused on getting it *right*, not getting there *first*.

There’s a lesson in that.

---

## Overdependence and Cognitive Decline

Here’s what is of utmost concern:

People are replacing understanding with shortcuts.  
Instead of learning how systems work or building deep skills, they’re learning how to write better prompts.

Btw, if AGI does arrive, we won’t need prompt engineers.  
And if it *doesn’t*, then a generation of surface-level users — who can’t solve problems without a tool — will be replaceable. Not by AI, but by others who kept learning.

There’s no value in being “good at using ChatGPT” if that’s all you can do.

---

## We Train the Ceiling

LLMs don’t invent new ideas.  
They remix ours.

Until we build something fundamentally different — something that *thinks* — these models will only be as good as the people using them.

If we stop thinking, creating, or exploring because “AI will handle it,” we’re just capping the ceiling on our own progress.

---

## The Bottom Line

Your AI assistant is incredible at pattern matching. It's a master of looking intelligent. But it doesn't think, doesn't understand, and doesn't know it exists.

The danger isn't that AI will replace human thinking.
The danger is that we'll forget what thinking actually is — and mistake performance for intelligence, fluency for understanding, and pattern matching for wisdom.

Don't let the illusion fool you. Keep thinking. Keep questioning. Keep creating.

Because if we don't, who will train the next generation of models?
